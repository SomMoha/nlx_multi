{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb813f49-f36c-49ab-bf63-de7c24dfacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import GPT2Tokenizer, AutoConfig\n",
    "from models.clip import ImageEncoder\n",
    "from models.gpt import GPT2LMHeadModel\n",
    "from utils import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aee70d6-42df-4e73-b2e5-f94e6b7ef92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'articles', 'comma_strip', 'contractions', 'manual_map', 'period_strip', 'prep_ans', 'proc_ans', 'proc_ques', 'process_digit_article', 'process_punctuation', 'punct', 're']\n"
     ]
    }
   ],
   "source": [
    "# class_exist = hasattr(data_utils,proc_ans)\n",
    "modlule_contents = dir(data_utils)\n",
    "print(modlule_contents)\n",
    "# print(class_exist)\n",
    "# print(type(data_utils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f95839f5-4db3-4924-a783-acbe3c453de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def change_requires_grad(model, req_grad):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = req_grad\n",
    "\n",
    "def load_checkpoint(ckpt_path, epoch):\n",
    "    model_name = 'nle_model_{}'.format(str(epoch))\n",
    "    tokenizer_name = 'nle_gpt2_tokenizer_0'\n",
    "    filename = 'ckpt_stats_' + str(epoch) + '.tar'\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(ckpt_path + tokenizer_name)  # load tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(ckpt_path + model_name).to(device)  # load pretrained_model with config\n",
    "    opt = torch.load(ckpt_path + filename)\n",
    "    optimizer = get_optimizer(model, learning_rate)\n",
    "    optimizer.load_state_dict(opt['optimizer_state_dict'])\n",
    "    start_epoch = opt['epoch'] + 1\n",
    "    scheduler_dic = opt['scheduler']\n",
    "    del opt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return tokenizer, model, optimizer, scheduler_dic, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52fb6a75-381b-4f29-abc0-ffa055b8b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained():\n",
    "    model_path = 'pretrained_model/pretrain_model'\n",
    "    tokenizer_path = 'pretrained_model/pretrain_tokenizer_0'\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)  # load tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)  # load pretrained_model with config\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, unwrapped_model, optimizer, tokenizer, scheduler, ckpt_path, **kwargs):\n",
    "    model_name = 'nle_model_{}'.format(str(epoch))\n",
    "    tokenizer_name = 'nle_gpt2_tokenizer_{}'.format(str(epoch))\n",
    "    filename = 'ckpt_stats_' + str(epoch) + '.tar'\n",
    "\n",
    "    if epoch == 0:\n",
    "        tokenizer.save_pretrained(ckpt_path + tokenizer_name)  # save tokenizer\n",
    "\n",
    "    unwrapped_model.save_pretrained(ckpt_path + model_name, save_function=accelerator.save)\n",
    "\n",
    "    opt = {'epoch': epoch,\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'scheduler': scheduler.state_dict(),\n",
    "           **kwargs}\n",
    "\n",
    "    torch.save(opt, ckpt_path + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c87a1a7c-41ad-48ae-a018-0ce5511ec600",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVQAXTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, transform, tokenizer, max_seq_len):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len  # question + <bos> The answer is <answer> because <explanation> <eos>\n",
    "        self.data = json.load(open(path, 'r'))\n",
    "        self.ids_list = list(self.data.keys())\n",
    "\n",
    "        for k, v in self.data.items():\n",
    "            if len(v['explanation']) > 1:  # some questions have more than one explanation\n",
    "                # duplicate them for loading. -1 because one explanation is already in ids_list\n",
    "                self.ids_list += [str(k)] * (len(v['explanation']) - 1)\n",
    "\n",
    "        self.index_tracker = {k: len(v['explanation']) - 1 for k, v in self.data.items()}\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        question_id = self.ids_list[i]\n",
    "        sample = self.data[question_id]\n",
    "        img_name = sample['image_name']\n",
    "        text_a = sample['question']  # question\n",
    "        answer = sample['answers']\n",
    "\n",
    "        exp_idx = self.index_tracker[question_id]  # the index of the explanation for questions with multiple explanations\n",
    "        if exp_idx > 0:\n",
    "            self.index_tracker[question_id] -= 1  # decrease usage\n",
    "\n",
    "        text_b = sample['explanation'][exp_idx]  # explanation\n",
    "\n",
    "        # tokenization process\n",
    "        q_segment_id, a_segment_id, e_segment_id = self.tokenizer.convert_tokens_to_ids(['<question>',\n",
    "                                                                                         '<answer>',\n",
    "                                                                                         '<explanation>'])\n",
    "        tokens = self.tokenizer.tokenize(text_a)\n",
    "        labels = [-100] * len(tokens)  # we do not want to predict the question, set to pad to ignore in XE\n",
    "        segment_ids = [q_segment_id] * len(tokens)\n",
    "        if isinstance(answer, str):\n",
    "            answer = [self.tokenizer.bos_token] + self.tokenizer.tokenize(\" the answer is \" + answer)\n",
    "        elif isinstance(answer, list) and len(answer)>0 and 'answer' in answer[0]:\n",
    "            answer = [self.tokenizer.bos_token] + self.tokenizer.tokenize(\" the answer is \" + answer[0]['answer'])\n",
    "        else:\n",
    "            print(\"other structure than ones handled!!\")\n",
    "        \n",
    "        answer_len = len(answer)\n",
    "        tokens_b = self.tokenizer.tokenize(\" because \" + text_b) + [self.tokenizer.eos_token]\n",
    "        exp_len = len(tokens_b)\n",
    "        tokens += answer + tokens_b\n",
    "        labels += [-100] + answer[1:] + tokens_b  # labels will be shifted in the pretrained_model, so for now set them same as tokens\n",
    "        segment_ids += [a_segment_id] * answer_len\n",
    "        segment_ids += [e_segment_id] * exp_len\n",
    "\n",
    "        if len(tokens) > self.max_seq_len:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            labels = labels[:self.max_seq_len]\n",
    "            segment_ids = segment_ids[:self.max_seq_len]\n",
    "\n",
    "        assert len(tokens) == len(segment_ids)\n",
    "        assert len(tokens) == len(labels)\n",
    "\n",
    "        seq_len = len(tokens)\n",
    "        padding_len = self.max_seq_len - seq_len\n",
    "        tokens = tokens + ([self.tokenizer.pad_token] * padding_len)\n",
    "        labels = labels + ([-100] * padding_len)\n",
    "\n",
    "        segment_ids += ([e_segment_id] * padding_len)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "        labels = [self.tokenizer.convert_tokens_to_ids(t) if t != -100 else t for t in labels]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "\n",
    "        folder = 'images/'\n",
    "        img_path = folder + img_name\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        qid = torch.LongTensor([int(question_id)])\n",
    "\n",
    "        return (img, qid, input_ids, labels, segment_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02eed1e3-1ca0-49f1-a3e0-0c604ec58ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, learning_rate):\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0}]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f67361c-acdc-4262-bf8f-50a69a82933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.2.crossattention.c_proj.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.5.crossattention.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.0.crossattention.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.4.crossattention.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.2.crossattention.bias', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.3.crossattention.bias', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.0.crossattention.q_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 2.00 GiB of which 0 bytes is free. Of the allocated memory 1.33 GiB is allocated by PyTorch, and 3.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m         model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m     51\u001b[0m         model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m---> 52\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m get_optimizer(model, learning_rate)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Setup Ready...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\transformers\\modeling_utils.py:2427\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2423\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2424\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2425\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2426\u001b[0m         )\n\u001b[1;32m-> 2427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mE:\\Download\\myProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 2.00 GiB of which 0 bytes is free. Of the allocated memory 1.33 GiB is allocated by PyTorch, and 3.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "finetune_pretrained = False  # if True, finetunes from the image captioning pretrained_model\n",
    "img_size = 224\n",
    "ckpt_path = 'ckpts/'\n",
    "nle_data_train_path = 'nle_data/nle_data_train.json'\n",
    "max_seq_len = 40\n",
    "load_from_epoch = None\n",
    "no_sample = True\n",
    "top_k = 0\n",
    "top_p = 0.9\n",
    "batch_size = 32  # per GPU\n",
    "num_train_epochs = 30\n",
    "weight_decay = 0\n",
    "learning_rate = 2e-5 if not finetune_pretrained else 1e-5\n",
    "gradient_accumulation_steps = 1\n",
    "start_epoch = 0\n",
    "temperature = 1\n",
    "\n",
    "image_encoder = ImageEncoder(device).to(device)\n",
    "change_requires_grad(image_encoder, False)\n",
    "\n",
    "if load_from_epoch is not None:\n",
    "    tokenizer, model, optimizer, scheduler_dic, start_epoch = load_checkpoint(ckpt_path, load_from_epoch)\n",
    "\n",
    "else:\n",
    "\n",
    "    if finetune_pretrained:\n",
    "        tokenizer, model = load_pretrained()\n",
    "        optimizer = get_optimizer(model, learning_rate)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "        orig_num_tokens = len(tokenizer.encoder)\n",
    "\n",
    "        num_new_tokens = tokenizer.add_special_tokens({'pad_token': '<pad>',\n",
    "                                                       'additional_special_tokens': ['<question>', '<answer>',\n",
    "                                                                                     '<explanation>']})\n",
    "\n",
    "        assert len(tokenizer) == orig_num_tokens + num_new_tokens\n",
    "        config = AutoConfig.from_pretrained('distilgpt2')\n",
    "\n",
    "        # Add configs\n",
    "        setattr(config, 'img_size', None)\n",
    "        setattr(config, 'max_seq_len', None)\n",
    "        config.img_size = img_size\n",
    "        config.max_seq_len = max_seq_len\n",
    "        config.add_cross_attention = True\n",
    "\n",
    "        model = GPT2LMHeadModel.from_pretrained('distilgpt2', config=config)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model = model.to(device)\n",
    "        optimizer = get_optimizer(model, learning_rate)\n",
    "\n",
    "print(\"Model Setup Ready...\")\n",
    "\n",
    "img_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = MultiVQAXTrainDataset(path=nle_data_train_path,\n",
    "                                      transform=img_transform,\n",
    "                                      tokenizer=tokenizer,\n",
    "                                      max_seq_len=max_seq_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "\n",
    "t_total = (len(train_loader) // gradient_accumulation_steps) * num_train_epochs\n",
    "warmup_steps = 0  # 0.10 * t_total\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "if load_from_epoch is not None:\n",
    "    scheduler.load_state_dict(scheduler_dic)\n",
    "\n",
    "for epoch in range(start_epoch, num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    accum_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "        img, _, input_ids, labels, segment_ids = batch\n",
    "\n",
    "        img_embeddings = image_encoder(img)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        past_key_values=None,\n",
    "                        attention_mask=None,\n",
    "                        token_type_ids=segment_ids,\n",
    "                        position_ids=None,\n",
    "                        encoder_hidden_states=img_embeddings,\n",
    "                        encoder_attention_mask=None,\n",
    "                        labels=labels,\n",
    "                        use_cache=False,\n",
    "                        return_dict=True)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        accum_loss += loss.item()\n",
    "\n",
    "        if step % gradient_accumulation_steps == 0 or step == len(train_loader) - 1:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.print(\"\\rEpoch {} / {}, Iter {} / {}, Loss: {:.3f}\".format(epoch,\n",
    "                                                                                   num_train_epochs,\n",
    "                                                                                   step, len(train_loader),\n",
    "                                                                                   accum_loss),\n",
    "                              end='          ')\n",
    "            accum_loss = 0\n",
    "\n",
    "\n",
    "    save_checkpoint(epoch, model, optimizer, tokenizer, scheduler, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9489cc3-51ef-4ca0-a81e-323bf3d237a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
