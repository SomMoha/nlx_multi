{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba638f-abaf-433f-a58a-1d54d0efa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator\n",
    "from models.clip import ImageEncoder\n",
    "from data_train import accelerator\n",
    "from utils import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968e07d-57cc-403f-8afe-6389683ead24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_get_scores(resFileExp, save_scores_pathExp, full_predictions, exp_predictions):\n",
    "\n",
    "    all_file = json.load(open(nle_data_test_path, 'r'))\n",
    "    \n",
    "    gt_answers = {}\n",
    "    for key,value in all_file.items():\n",
    "        gt_answers[int(key)] = data_utils.proc_ans(value['answers'])\n",
    "        \n",
    "    pred_answers = {}\n",
    "    for item in full_predictions:\n",
    "        pred_answers[item['image_id']] = item['caption'].split(\"because\")[0].strip()\n",
    "        \n",
    "    correct_keys = []\n",
    "    for key,value in pred_answers.items():\n",
    "        gt_answer = gt_answers[key]\n",
    "        # to measure accuracy for VQA, please change \"==\" to \"in\" (if value in gt_answer:)\n",
    "        # you need to also change the proc_ans funtion in utils/data_uitls.py to return: list(ans_prob_dict.keys())\n",
    "        if value == gt_answer:\n",
    "            correct_keys.append(key)\n",
    "            \n",
    "            \n",
    "    exp_preds = [item for item in exp_predictions if item['image_id'] in correct_keys]\n",
    "\n",
    "    with open(resFileExp, 'w') as w:\n",
    "        json.dump(exp_preds, w)\n",
    "        \n",
    "    coco = COCO(annFileExp)\n",
    "    cocoRes = coco.loadRes(resFileExp)\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    cocoEval.evaluate()\n",
    "    \n",
    "    with open(save_scores_pathExp, 'w') as w:\n",
    "        json.dump(cocoEval.eval, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115753bc-e361-4455-960e-37549414f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVQAXEvalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, transform, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len  # question + <bos> The answer is <answer> because <explanation> <eos>\n",
    "        self.data = json.load(open(path, 'r'))\n",
    "        self.ids_list = list(self.data.keys())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        quention_id = self.ids_list[i]\n",
    "        sample = self.data[quention_id]\n",
    "        img_name = sample['image_name']\n",
    "        text_a = data_utils.proc_ques(sample['question'])  # question\n",
    "\n",
    "        # tokenization process\n",
    "        q_segment_id, a_segment_id, e_segment_id = self.tokenizer.convert_tokens_to_ids(\n",
    "            ['<question>', '<answer>', '<explanation>'])\n",
    "        tokens = self.tokenizer.tokenize(text_a)\n",
    "        segment_ids = [q_segment_id] * len(tokens)\n",
    "\n",
    "        answer = [self.tokenizer.bos_token] + self.tokenizer.tokenize(\" the answer is\")\n",
    "        answer_len = len(answer)\n",
    "        tokens += answer\n",
    "\n",
    "        segment_ids += [a_segment_id] * answer_len\n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "\n",
    "        folder = 'images/'  \n",
    "        img_path = folder + img_name\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        qid = torch.LongTensor([int(quention_id)])\n",
    "\n",
    "        return (img, qid, input_ids, segment_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5456a8-7861-4002-b197-041e3122fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequences(model, tokenizer, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    results_exp = []\n",
    "    results_full = []\n",
    "    SPECIAL_TOKENS = ['<|endoftext|>', '<pad>', '<question>', '<answer>', '<explanation>']\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    because_token = tokenizer.convert_tokens_to_ids('Ä because')\n",
    "    max_len = 20\n",
    "    \n",
    "    for i,batch in enumerate(loader):\n",
    "        \n",
    "        current_output = []\n",
    "        batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "        img, img_id, input_ids, segment_ids = batch\n",
    "        img_embeddings = image_encoder(img)\n",
    "        always_exp = False\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for step in range(max_len + 1):\n",
    "                \n",
    "                if step == max_len:\n",
    "                    break\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, \n",
    "                                past_key_values=None, \n",
    "                                attention_mask=None, \n",
    "                                token_type_ids=segment_ids, \n",
    "                                position_ids=None, \n",
    "                                encoder_hidden_states=img_embeddings, \n",
    "                                encoder_attention_mask=None, \n",
    "                                labels=None, \n",
    "                                use_cache=False, \n",
    "                                return_dict=True)\n",
    "                \n",
    "                lm_logits = outputs.logits \n",
    "                logits = lm_logits[0, -1, :] / temperature\n",
    "                logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                prev = torch.topk(probs, 1)[1] if no_sample else torch.multinomial(probs, 1)\n",
    "                \n",
    "                if prev.item() in special_tokens_ids:\n",
    "                    break\n",
    "                \n",
    "                # take care of when to start the <explanation> token\n",
    "                if not always_exp:\n",
    "                    \n",
    "                    if prev.item() != because_token:\n",
    "                        new_segment = special_tokens_ids[-2]   # answer segment\n",
    "                    else:\n",
    "                        new_segment = special_tokens_ids[-1]   # explanation segment\n",
    "                        always_exp = True\n",
    "                else:\n",
    "                    new_segment = special_tokens_ids[-1]   # explanation segment\n",
    "                    \n",
    "                new_segment = torch.LongTensor([new_segment]).to(device)\n",
    "                current_output.append(prev.item())\n",
    "                input_ids = torch.cat((input_ids, prev.unsqueeze(0)), dim = 1)\n",
    "                segment_ids = torch.cat((segment_ids, new_segment.unsqueeze(0)), dim = 1)\n",
    "                \n",
    "        decoded_sequences = tokenizer.decode(current_output, skip_special_tokens=True).lstrip()\n",
    "        results_full.append({\"image_id\": img_id.item(), \"caption\": decoded_sequences})\n",
    "        \n",
    "        if 'because' in decoded_sequences:\n",
    "            cut_decoded_sequences = decoded_sequences.split('because')[-1].strip()\n",
    "        else:\n",
    "            cut_decoded_sequences = \" \".join(decoded_sequences.split()[2:])\n",
    "        \n",
    "        results_exp.append({\"image_id\": img_id.item(), \"caption\": cut_decoded_sequences})\n",
    "        print(\"\\rEvaluation: Finished {}/{}\".format(i, len(loader)), end='          ')\n",
    "            \n",
    "    return results_full, results_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2b733-837d-4447-8841-ee21bd7acb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_pretrained = False\n",
    "eval_batch_size = 1\n",
    "img_size = 224\n",
    "ckpt_path = 'ckpts/'\n",
    "caption_save_path = 'cococaption/results/' \n",
    "annFileExp = 'cococaption/annotations/multiVqa_test_annot_exp.json'\n",
    "annFileFull = 'cococaption/annotations/multiVqa_test_annot_full.json'\n",
    "nle_data_val_path = 'nle_data/nle_data_val.json'\n",
    "# nle_data_test_path = 'nle.data/nle_data_test.json'\n",
    "max_seq_len = 40\n",
    "load_from_epoch = None\n",
    "no_sample = True\n",
    "top_k = 0\n",
    "top_p = 0.9\n",
    "batch_size = 32  # per GPU\n",
    "num_train_epochs = 30\n",
    "weight_decay = 0\n",
    "learning_rate = 2e-5 if not finetune_pretrained else 1e-5\n",
    "gradient_accumulation_steps = 1\n",
    "start_epoch = 0\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2399c-f129-484d-95e2-5235d147fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = ImageEncoder(device).to(device)\n",
    "change_requires_grad(image_encoder, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d15ea-5a46-4cdc-bf25-3140c1fc8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    " if accelerator.is_main_process:\n",
    "        results_full, results_exp = sample_sequences(unwrapped_model, tokenizer, test_loader)\n",
    "\n",
    "        resFileExp = caption_save_path + 'captions_exp_' + str(epoch) + '.json'\n",
    "        unf_resFileExp = caption_save_path + 'unf_captions_exp_' + str(epoch) + '.json'\n",
    "        unf_resFileFull = caption_save_path + 'unf_captions_full_' + str(epoch) + '.json'\n",
    "        save_scores_pathExp = caption_save_path + 'scores_exp_' + str(epoch) + '.json'\n",
    "\n",
    "        with open(unf_resFileExp, 'w') as w:\n",
    "            json.dump(results_exp, w)\n",
    "\n",
    "        with open(unf_resFileFull, 'w') as w:\n",
    "            json.dump(results_full, w)\n",
    "\n",
    "    img_transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    val_dataset = MultiVQAXEvalDataset(path = nle_data_val_path,\n",
    "                                  transform = img_transform,\n",
    "                                  tokenizer = tokenizer,\n",
    "                                  max_seq_len = max_seq_len)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size = 1,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True)\n",
    "    \"\"\"\n",
    "    test_dataset = VQAXEvalDataset(path=nle_data_test_path,\n",
    "                                   transform=img_transform,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   max_seq_len=max_seq_len)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=True)\n",
    "    \"\"\"\n",
    "\n",
    "    # unfiltered results\n",
    "    # get_scores(annFileExp, unf_resFileExp, save_scores_pathExp)\n",
    "\n",
    "    # filtered results\n",
    "    # filter_and_get_scores(resFileExp, save_scores_pathExp, results_full, results_exp) ////\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea06a7-6f77-4996-9ee5-5a20fbe150a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80297171-1ea2-45de-b122-56b305591a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
